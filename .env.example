# =============================================================================
# Aura AI-Driven 3D Model Generation - Environment Configuration Template
# =============================================================================
# 
# Copy this file to .env in the root directory and fill in your values
# DO NOT commit the .env file to version control!
#
# Quick Start:
# 1. cp .env.example .env
# 2. Edit .env with your API keys
# 3. Start the backend: uvicorn backend.main:app --reload
# 4. Start the frontend: cd frontend/static && npm run dev
# =============================================================================

# -----------------------------------------------------------------------------
# OpenAI Configuration (Primary AI Provider - RECOMMENDED)
# -----------------------------------------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-key-here
# Model selection (gpt-4o recommended for best results)
# Options: gpt-4o, gpt-4-turbo, gpt-4, gpt-3.5-turbo
OPENAI_MODEL=gpt-4o

# Temperature controls creativity (0.0-2.0, default: 0.7)
# Lower = more focused, Higher = more creative
OPENAI_TEMPERATURE=0.7

# Maximum tokens for responses (default: 4096)
# Higher = more detailed responses, but slower and more expensive
OPENAI_MAX_TOKENS=4096

# -----------------------------------------------------------------------------
# Backend Server Configuration
# -----------------------------------------------------------------------------
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8001
BACKEND_WORKERS=1

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Environment (development, production)
ENVIRONMENT=development

# Debug mode (true/false)
DEBUG_MODE=false

# -----------------------------------------------------------------------------
# Alternative AI Providers (Optional)
# -----------------------------------------------------------------------------

# LM Studio (Local AI - Free Alternative)
# Download from: https://lmstudio.ai/
LM_STUDIO_URL=http://localhost:1234/v1/chat/completions
LM_STUDIO_MODEL=llama-3.1-8b-instruct
LM_STUDIO_TEMPERATURE=0.7
LM_STUDIO_MAX_TOKENS=1000

# Anthropic Claude (Optional)
# Get API key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=your-anthropic-key-here
# ANTHROPIC_MODEL=claude-3-sonnet-20240229
# ANTHROPIC_TEMPERATURE=0.7
# ANTHROPIC_MAX_TOKENS=1000

# Google Gemini (Optional)
# Get API key from: https://makersuite.google.com/app/apikey
# GOOGLE_API_KEY=your-google-ai-key-here
# GOOGLE_MODEL=gemini-pro
# GOOGLE_TEMPERATURE=0.7
# GOOGLE_MAX_TOKENS=1000

# Hugging Face (Optional)
# Get API key from: https://huggingface.co/settings/tokens
# HUGGINGFACE_API_KEY=your-huggingface-key-here
# HUGGINGFACE_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
# HUGGINGFACE_TEMPERATURE=0.7
# HUGGINGFACE_MAX_TOKENS=1000

# Azure OpenAI (Optional)
# AZURE_OPENAI_API_KEY=your-azure-key-here
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_DEPLOYMENT=your-deployment-name
# AZURE_TEMPERATURE=0.7
# AZURE_MAX_TOKENS=1000

# Ollama (Local AI - Free Alternative)
# Download from: https://ollama.ai/
# OLLAMA_URL=http://localhost:11434/api/generate
# OLLAMA_MODEL=llama3.1
# OLLAMA_TEMPERATURE=0.7
# OLLAMA_MAX_TOKENS=1000

# -----------------------------------------------------------------------------
# AI Provider Selection (Optional)
# -----------------------------------------------------------------------------
# Explicitly select which AI provider to use
# Options: openai, anthropic, google_ai, azure_openai, huggingface, lm_studio, ollama
# If not set, auto-selects first available provider with valid API key
# AI_PROVIDER=openai

# Max retries for AI requests (default: 3)
AI_MAX_RETRIES=3

# Retry delay in seconds (default: 2.0)
AI_RETRY_DELAY=2.0

# -----------------------------------------------------------------------------
# Directory Configuration
# -----------------------------------------------------------------------------
OUTPUT_DIR=./output
MODELS_DIR=./models
CACHE_DIR=./cache
LOGS_DIR=./logs

# -----------------------------------------------------------------------------
# Blender Integration (Optional - for advanced rendering)
# -----------------------------------------------------------------------------
# Path to Blender executable (leave empty to disable)
# BLENDER_PATH=C:\Program Files\Blender Foundation\Blender 4.5\blender.exe  # Windows
# BLENDER_PATH=/Applications/Blender.app/Contents/MacOS/Blender  # macOS
# BLENDER_PATH=/usr/bin/blender  # Linux

BLENDER_BACKGROUND=true
BLENDER_TIMEOUT=300

# -----------------------------------------------------------------------------
# Frontend Configuration
# -----------------------------------------------------------------------------
# API URL for frontend to connect to backend
VITE_API_URL=http://localhost:8001/api

# -----------------------------------------------------------------------------
# Security & Performance
# -----------------------------------------------------------------------------
# Maximum memory usage in GB
MAX_MEMORY_GB=8

# Enable metrics collection
METRICS_ENABLED=true
METRICS_PORT=9090

# Health check interval in seconds
HEALTH_CHECK_INTERVAL=30

# Performance monitoring
PERFORMANCE_MONITORING=true
GPU_MONITORING=true
MEMORY_MONITORING=true

# -----------------------------------------------------------------------------
# Advanced Options
# -----------------------------------------------------------------------------
# Sandbox mode (disables external AI calls, uses fallback logic only)
SANDBOX_MODE=false

# Sandbox server configuration (if using sandbox mode)
# SANDBOX_SERVER_HOST=0.0.0.0
# SANDBOX_SERVER_PORT=8003
# SANDBOX_SERVER_URL=http://localhost:8003

# -----------------------------------------------------------------------------
# Production Recommendations
# -----------------------------------------------------------------------------
# For production deployment, consider:
# 1. Set ENVIRONMENT=production
# 2. Set DEBUG_MODE=false
# 3. Set LOG_LEVEL=WARNING or ERROR
# 4. Use stronger API rate limiting
# 5. Enable metrics and monitoring
# 6. Use HTTPS for all connections
# 7. Implement API key rotation
# 8. Set up proper backup for OUTPUT_DIR
# =============================================================================
