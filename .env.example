# ====================================================================
# Aura AI Design Studio - Environment Configuration
# ====================================================================
#
# This file contains all configuration options for the Aura system.
# Copy this to .env and fill in your values.
#
# Quick Start:
# 1. Choose your AI provider (see AI PROVIDER CONFIGURATION section)
# 2. Add the corresponding API key
# 3. Configure Blender path if available
# 4. Start the backend: python -m uvicorn backend.main:app
# ====================================================================

# --------------------------------------------------------------------
# SYSTEM CONFIGURATION
# --------------------------------------------------------------------
ENVIRONMENT=development
DEBUG_MODE=true
LOG_LEVEL=INFO
LOG_FORMAT=[%(asctime)s] %(levelname)s %(message)s

# --------------------------------------------------------------------
# AI PROVIDER CONFIGURATION
# --------------------------------------------------------------------
# The system supports multiple AI providers with automatic failover.
# Configure one or more providers below. The system will automatically
# select the best available provider based on priority.
#
# Priority order (when AI_PROVIDER not set):
# 1. OpenAI  2. Anthropic  3. Google AI  4. Azure OpenAI
# 5. Hugging Face  6. LM Studio  7. Ollama
#
# To force a specific provider, set AI_PROVIDER to one of:
# lm_studio, openai, google_ai, anthropic, huggingface, azure_openai, ollama
# --------------------------------------------------------------------

# Explicit provider selection (optional)
# AI_PROVIDER=openai

# === LM Studio (Local, No API Key Required) ===
# Download from: https://lmstudio.ai
# Load a model (e.g., Llama 3.1 8B)
# Start server on port 1234
LM_STUDIO_URL=http://localhost:1234/v1/chat/completions
LM_STUDIO_MODEL=llama-3.1-8b-instruct
LM_STUDIO_TEMPERATURE=0.7
LM_STUDIO_MAX_TOKENS=1000

# === OpenAI ===
# Get API key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4
# OPENAI_TEMPERATURE=0.7
# OPENAI_MAX_TOKENS=1000

# === Google AI (Gemini) ===
# Get API key from: https://makersuite.google.com/app/apikey
# GOOGLE_API_KEY=...
# GOOGLE_MODEL=gemini-pro
# GOOGLE_TEMPERATURE=0.7
# GOOGLE_MAX_TOKENS=1000

# === Anthropic (Claude) ===
# Get API key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_MODEL=claude-3-sonnet-20240229
# ANTHROPIC_TEMPERATURE=0.7
# ANTHROPIC_MAX_TOKENS=1000

# === Hugging Face ===
# Get API key from: https://huggingface.co/settings/tokens
# HUGGINGFACE_API_KEY=hf_...
# HUGGINGFACE_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
# HUGGINGFACE_TEMPERATURE=0.7
# HUGGINGFACE_MAX_TOKENS=1000

# === Azure OpenAI ===
# Get from Azure Portal: https://portal.azure.com
# AZURE_OPENAI_API_KEY=...
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_DEPLOYMENT=your-deployment-name
# AZURE_TEMPERATURE=0.7
# AZURE_MAX_TOKENS=1000

# === Ollama (Local, No API Key Required) ===
# Download from: https://ollama.ai
# Install a model: ollama pull llama3.1
# OLLAMA_URL=http://localhost:11434/api/generate
# OLLAMA_MODEL=llama3.1
# OLLAMA_TEMPERATURE=0.7
# OLLAMA_MAX_TOKENS=1000

# --------------------------------------------------------------------
# BLENDER CONFIGURATION
# --------------------------------------------------------------------
# For real 3D model generation, Blender must be installed.
# If not set, the system will auto-detect common installation paths.
#
# Linux: /usr/bin/blender
# macOS: /Applications/Blender.app/Contents/MacOS/Blender
# Windows: C:\Program Files\Blender Foundation\Blender 4.5\blender.exe
# --------------------------------------------------------------------

# BLENDER_PATH=/usr/bin/blender
BLENDER_BACKGROUND=true
BLENDER_TIMEOUT=300

# --------------------------------------------------------------------
# BACKEND CONFIGURATION
# --------------------------------------------------------------------
BACKEND_HOST=localhost
BACKEND_PORT=8001
BACKEND_WORKERS=1

# --------------------------------------------------------------------
# DIRECTORIES
# --------------------------------------------------------------------
OUTPUT_DIR=./output
MODELS_DIR=./models
CACHE_DIR=./cache
LOGS_DIR=./logs

# --------------------------------------------------------------------
# SANDBOX MODE
# --------------------------------------------------------------------
# Set to true to use Hugging Face API instead of local LM Studio
# (Useful for environments without GPU or when LM Studio unavailable)
SANDBOX_MODE=false
SANDBOX_SERVER_PORT=8003

# --------------------------------------------------------------------
# MONITORING & PERFORMANCE
# --------------------------------------------------------------------
METRICS_ENABLED=true
METRICS_PORT=9090
HEALTH_CHECK_INTERVAL=30
PERFORMANCE_MONITORING=true
GPU_MONITORING=true
MEMORY_MONITORING=true
MAX_MEMORY_GB=8
